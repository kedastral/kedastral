# ğŸ“˜ README.md â€” *Kedastral*

> **Kedastral** â€” *Predict tomorrow's load, scale today.*

[![Release](https://img.shields.io/github/v/release/kedastral/kedastral?color=blue)](https://github.com/kedastral/kedastral/releases/latest)
[![Go Reference](https://pkg.go.dev/badge/github.com/kedastral/kedastral.svg)](https://pkg.go.dev/github.com/kedastral/kedastral)
[![Go Report Card](https://goreportcard.com/badge/github.com/kedastral/kedastral)](https://goreportcard.com/report/github.com/kedastral/kedastral)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

---

## ğŸ§­ Overview

**Kedastral** is an open-source, domain-agnostic **predictive autoscaling companion for [KEDA](https://keda.sh/)**, written in **Go**.

It enables Kubernetes workloads to **scale proactively**, not just reactively, by **forecasting future demand** (for example, request rate, queue depth, or events) and translating that forecast into desired replica counts **before** resource metrics like CPU or RPS spike.

Where **KEDA** reacts to what *has already happened*, **Kedastral** predicts *what will happen next* â€” keeping applications responsive, stable, and cost-efficient during sudden traffic surges.

---

## ğŸš€ Current Features (v0.1 MVP)

| Feature | Description | Status |
|----------|-------------|--------|
| ğŸ”® **Predictive scaling** | Forecast short-term demand and set replica counts ahead of time | âœ… Implemented |
| âš™ï¸ **KEDA-native integration** | Implements the official KEDA **External Scaler** gRPC interface | âœ… Implemented |
| ğŸ“ˆ **Prometheus adapter** | Pull metrics from Prometheus for forecasting | âœ… Implemented |
| ğŸ’¾ **Storage backends** | In-memory (default) and Redis for HA deployments | âœ… Implemented |
| ğŸ§  **Baseline forecasting model** | Statistical baseline with quantile-based prediction | âœ… Implemented |
| ğŸ“Š **ARIMA forecasting model** | Time-series forecasting for trending/seasonal workloads | âœ… Implemented |
| ğŸ§  **Built in Go** | Fast, efficient, minimal footprint; deployable as static binaries or containers | âœ… Implemented |
| ğŸ§± **Extensible interfaces** | Well-defined interfaces for adapters and models | âœ… Implemented |
| ğŸ” **Data stays local** | All forecasting and scaling happen *inside* your cluster | âœ… Implemented |
| ğŸ“Š **Prometheus metrics** | Exposes metrics for monitoring forecast health | âœ… Implemented |
| ğŸ³ **Docker support** | Dockerfiles for containerized deployment | âœ… Implemented |
| ğŸ§ª **Comprehensive tests** | 81 unit tests covering core functionality | âœ… Implemented |

### ğŸ”® Planned Features

- **Declarative CRDs** - Kubernetes-native configuration (`ForecastPolicy`, `DataSource`)
- **Additional adapters** - Kafka, HTTP APIs, and custom data sources
- **Advanced ML models** - Prophet, SARIMA, and custom model support
- **Helm charts** - Easy deployment via Helm
- **Grafana dashboards** - Pre-built dashboards for visualization

---

## ğŸ“Š Forecasting Models

Kedastral supports two forecasting models, selectable via the `--model` flag:

### Baseline (Default)
- **Algorithm**: Linear trend + momentum + multi-level seasonality (minute-of-hour + hour-of-day)
- **Training**: Uses historical window to learn seasonal patterns (requires 2-3 pattern repetitions)
- **Best for**: Workloads with recurring patterns (hourly/daily cycles)
- **Pros**: Fast, adaptive blending of trend and seasonality, handles intra-hour and daily patterns
- **Cons**: Needs 2-3 hours of data for optimal accuracy
- **Configuration**: `--model=baseline` (default)

### ARIMA
- **Algorithm**: AutoRegressive Integrated Moving Average (pure Go)
- **Training**: Required (uses historical window)
- **Best for**: Workloads with trends, seasonality, or autocorrelation
- **Pros**: Better accuracy for complex patterns
- **Cons**: Requires training data, slower startup
- **Configuration**: `--model=arima --arima-p=1 --arima-d=1 --arima-q=1`

**Model Comparison**:

| Feature              | Baseline | ARIMA |
|----------------------|----------|-------|
| Training time        | None     | ~15Î¼s per 1K points |
| Prediction time      | <10ms    | <1Î¼s for 30 steps |
| Memory overhead      | ~1MB     | ~5MB |
| Handles trends       | âŒ       | âœ…    |
| Handles seasonality  | Basic    | âœ…    |
| Training data needed | No       | Yes   |
| Recommended for      | Stable workloads | Complex patterns |

**Example usage**:

```bash
# Baseline (default)
./forecaster --workload=my-api --model=baseline

# ARIMA with auto parameters (default: p=1, d=1, q=1)
./forecaster --workload=my-api --model=arima

# ARIMA with custom parameters
./forecaster --workload=my-api --model=arima --arima-p=2 --arima-d=1 --arima-q=2
```

**ARIMA Parameters**:
- **p** (AR order): How many past values to use (1-3 typical)
- **d** (differencing): Trend removal (0=none, 1=linear, 2=quadratic)
- **q** (MA order): How many past errors to use (1-3 typical)
- **Auto (0)**: Defaults to 1 for all parameters (ARIMA(1,1,1))

---

## ğŸ’¡ Example Use Cases

Kedastral is **domain-neutral**. You can use it for any workload that shows predictable or event-driven traffic patterns:

| Domain | Typical signals | Scaling goal |
|---------|------------------|--------------|
| E-commerce | request rate, promotions, time of day | scale before sales campaigns |
| Video streaming | viewer counts, release schedule | pre-scale for new show launches |
| Banking & fintech | batch job schedules, queue lag | prepare for end-of-month loads |
| IoT ingestion | connected devices count | absorb telemetry spikes gracefully |
| SaaS APIs & gaming | RPS, active sessions, time windows | prevent latency from scaling delays |

---

## ğŸ—ï¸ Architecture Overview

Kedastral currently consists of **two main components**, both implemented in **Go** for performance and operational simplicity.

### 1. **Forecaster** (`cmd/forecaster`)

- Collects recent metrics from **Prometheus** using configurable queries
- **Warm-start capability**: On startup, queries Prometheus for full historical window (e.g., 3 hours) to immediately start with learned patterns
- Uses a **baseline forecasting model** (trend + momentum + multi-level seasonality) to predict short-term load
- Translates predicted load into **desired replica counts** using a configurable capacity policy
- Stores forecasts in memory and exposes them via HTTP API (`/forecast/current`)
- Exposes Prometheus metrics for monitoring (`/metrics`)
- Health check endpoint (`/healthz`)

### 2. **Scaler** (`cmd/scaler`)

- Implements the [KEDA External Scaler gRPC API](https://keda.sh/docs/latest/concepts/external-scalers/)
- Periodically queries the Forecaster via HTTP to fetch the latest forecast
- Selects appropriate replica count based on configured **lead time** (proactive scaling window)
  - Default: 5 minutes (good for gradual changes)
  - Recommended: 10-15 minutes for workloads with predictable spikes
  - Takes MAX over the lead-time window for proactive scale-up
- Returns **desired replicas** to KEDA via gRPC interface
- Exposes health check and metrics endpoints

The two components form a closed feedback loop:

```
Prometheus â†’ Forecaster â†’ HTTP â†’ Scaler â†’ gRPC â†’ KEDA â†’ HPA â†’ Workload
```

---

### ğŸ§© Component Diagram (ASCII)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Metrics Sources   â”‚
â”‚ (Prometheus, etc.) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kedastral         â”‚
â”‚  Forecast Engine   â”‚  (Go)
â”‚  â€¢ Collects data   â”‚
â”‚  â€¢ Forecasts load  â”‚
â”‚  â€¢ Outputs replicasâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ REST/gRPC
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kedastral Scaler  â”‚  (Go, gRPC)
â”‚  â€¢ KEDA plugin     â”‚
â”‚  â€¢ Reports replicasâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        KEDA        â”‚
â”‚   (HPA controller) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Target Deployment  â”‚
â”‚   (User workload)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§­ Mermaid Diagram

```mermaid
flowchart TD
    A["Metrics Sources (Prometheus Â· Kafka Â· HTTP Â· Custom)"] --> B["Forecast Engine (Go)"]
    B --> C["Kedastral External Scaler (Go, gRPC)"]
    C --> D["KEDA Operator"]
    D --> E["Horizontal Pod Autoscaler"]
    E --> F["Target Deployment (User workload)"]
    D -.->|Reactive metrics| B
```

---

## ğŸ’¾ Storage Backends

Kedastral supports two storage backends for forecast snapshots, allowing you to choose between simplicity and high availability:

### In-Memory Storage (Default)
- **Best for**: Single forecaster instance, development, testing
- **Pros**: Zero dependencies, fast, simple setup
- **Cons**: No persistence across restarts, no HA support
- **Configuration**: `--storage=memory` (default)

```bash
# Uses in-memory storage by default
./forecaster --workload=my-api --metric=http_rps
```

### Redis Storage
- **Best for**: Multi-instance forecasters, production HA deployments, persistence
- **Pros**: Shared state across replicas, TTL-based expiration, horizontal scaling
- **Cons**: Requires Redis server, additional network dependency
- **Configuration**: `--storage=redis --redis-addr=HOST:PORT`

```bash
# Using Redis storage
./forecaster --storage=redis \
  --redis-addr=redis:6379 \
  --redis-ttl=1h \
  --workload=my-api \
  --metric=http_rps
```

**Redis Configuration Options:**
- `--storage=redis` - Enable Redis backend
- `--redis-addr=HOST:PORT` - Redis server address (default: localhost:6379)
- `--redis-password=SECRET` - Redis password (optional)
- `--redis-db=N` - Redis database number (default: 0)
- `--redis-ttl=DURATION` - Snapshot TTL (default: 30m)

**Example HA Deployment:**
See [`examples/deployment-redis.yaml`](examples/deployment-redis.yaml) for a complete Kubernetes deployment with:
- Redis for persistent storage
- 2+ forecaster replicas sharing Redis
- Scaler consuming forecasts from HA forecasters

---

## ğŸ§  How It Works

1. **Data Collection:** Kedastralâ€™s adapters pull short-term metrics and contextual features from your chosen data sources.
2. **Forecasting:** The engine runs a forecasting model to estimate load (RPS, queue length, etc.) for the next few minutes.
3. **Replica Calculation:** Using the configured capacity model, Kedastral computes how many pods will be required to handle that future load.
4. **Integration with KEDA:**
   - The Kedastral External Scaler exposes the forecast as a metric via gRPC.
   - KEDA reads it and updates the Horizontal Pod Autoscaler (HPA).
   - Your workload scales *before* demand arrives.

---

## ğŸš€ Quick Start

### Building from Source

```bash
# Clone the repository
git clone https://github.com/HatiCode/kedastral.git
cd kedastral

# Build both forecaster and scaler
make build

# Or build individually
make forecaster
make scaler

# Run tests
make test
```

### Running Locally

#### 1. Start the Forecaster

The forecaster generates predictions and exposes them via HTTP:

```bash
./bin/forecaster \
  -workload=my-api \
  -metric=http_rps \
  -prom-url=http://localhost:9090 \
  -prom-query='sum(rate(http_requests_total{service="my-api"}[1m]))' \
  -target-per-pod=100 \
  -headroom=1.2 \
  -min=2 \
  -max=50 \
  -lead-time=5m \
  -log-level=info
```

Check the forecast:
```bash
curl "http://localhost:8081/forecast/current?workload=my-api"
```

#### 2. Start the Scaler

The scaler implements the KEDA External Scaler gRPC interface:

```bash
./bin/scaler \
  -forecaster-url=http://localhost:8081 \
  -lead-time=5m \
  -log-level=info
```

The scaler exposes:
- gRPC on `:50051` for KEDA
- HTTP metrics on `:8082`

#### 3. Configure KEDA

Apply a ScaledObject to connect KEDA to Kedastral:

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: my-api-scaledobject
spec:
  scaleTargetRef:
    name: my-api
    kind: Deployment
  pollingInterval: 30
  minReplicaCount: 2
  maxReplicaCount: 50
  triggers:
    - type: external
      metadata:
        scalerAddress: kedastral-scaler:50051
        workload: my-api
```

### Deploying to Kubernetes

See the [examples/](./examples/) directory for complete Kubernetes deployment manifests:

- **[examples/deployment.yaml](./examples/deployment.yaml)** - Complete deployment for forecaster and scaler
- **[examples/scaled-object.yaml](./examples/scaled-object.yaml)** - KEDA ScaledObject configuration
- **[examples/README.md](./examples/README.md)** - Detailed usage guide with configuration tables and troubleshooting

Quick deploy:
```bash
kubectl apply -f examples/deployment.yaml
kubectl apply -f examples/scaled-object.yaml
```

---

## ğŸ§° Current Tech Stack

| Component | Technology | Status |
|------------|-------------|---------|
| Core language | **Go** (â‰¥1.25) | âœ… |
| Forecaster API | REST (HTTP) | âœ… |
| Scaler API | gRPC (KEDA External Scaler protocol) | âœ… |
| Forecast model | Baseline (statistical) | âœ… |
| Metrics adapter | Prometheus | âœ… |
| Storage | In-memory | âœ… |
| Observability | Prometheus metrics | âœ… |
| Testing | Go testing framework (81 tests) | âœ… |
| Deployment | Kubernetes manifests + Dockerfiles | âœ… |

**Planned:** Redis storage, Helm charts, additional adapters (Kafka, HTTP), ML models (Prophet, ARIMA), Grafana dashboards

---

## ğŸ§± Current Project Structure

```
kedastral/
â”œâ”€ cmd/
â”‚  â”œâ”€ forecaster/          # Forecaster binary and subpackages
â”‚  â”‚  â”œâ”€ main.go
â”‚  â”‚  â”œâ”€ forecaster.go
â”‚  â”‚  â”œâ”€ config/           # Configuration parsing
â”‚  â”‚  â”œâ”€ logger/           # Structured logging
â”‚  â”‚  â”œâ”€ metrics/          # Prometheus metrics
â”‚  â”‚  â””â”€ router/           # HTTP routes
â”‚  â””â”€ scaler/              # Scaler binary and subpackages
â”‚     â”œâ”€ main.go
â”‚     â”œâ”€ scaler.go
â”‚     â”œâ”€ config/           # Configuration parsing
â”‚     â”œâ”€ logger/           # Structured logging
â”‚     â”œâ”€ metrics/          # Prometheus metrics
â”‚     â””â”€ router/           # HTTP routes
â”œâ”€ pkg/
â”‚  â”œâ”€ adapters/            # Prometheus adapter
â”‚  â”œâ”€ models/              # Baseline forecasting model
â”‚  â”œâ”€ capacity/            # Replica calculation logic
â”‚  â”œâ”€ features/            # Feature engineering
â”‚  â”œâ”€ storage/             # In-memory snapshot storage
â”‚  â”œâ”€ httpx/               # HTTP server utilities
â”‚  â””â”€ api/externalscaler/  # KEDA External Scaler protobuf
â”œâ”€ examples/               # Kubernetes deployment examples
â”‚  â”œâ”€ deployment.yaml      # Complete deployment manifests
â”‚  â”œâ”€ scaled-object.yaml   # KEDA ScaledObject example
â”‚  â””â”€ README.md            # Detailed usage guide
â”œâ”€ docs/                   # Design documentation
â”‚  â”œâ”€ capacity-planner.md
â”‚  â”œâ”€ cli-design.md
â”‚  â””â”€ forecaster-store-interface.md
â”œâ”€ test/integration/       # Integration tests
â”œâ”€ Dockerfile.forecaster   # Forecaster container image
â”œâ”€ Dockerfile.scaler       # Scaler container image
â”œâ”€ Makefile                # Build automation
â””â”€ LICENSE (Apache-2.0)
```

---

## ğŸ”§ Installation

### Prerequisites

- Go 1.25 or later (for building from source)
- Kubernetes cluster (v1.20+)
- KEDA installed ([installation guide](https://keda.sh/docs/latest/deploy/))
- Prometheus running in the cluster

### From Source

```bash
# Clone and build
git clone https://github.com/HatiCode/kedastral.git
cd kedastral
make build

# Deploy to Kubernetes
kubectl apply -f examples/deployment.yaml
kubectl apply -f examples/scaled-object.yaml
```

### Using Makefile

```bash
make build           # Build both forecaster and scaler
make test            # Run all tests
make test-coverage   # Run tests with coverage report
make clean           # Remove build artifacts
make help            # Show all available targets
```

See the [examples/README.md](./examples/README.md) for detailed deployment instructions and configuration options.

---

## ğŸ“š Documentation

### API Documentation

Full Go package documentation is available at [pkg.go.dev](https://pkg.go.dev/github.com/HatiCode/kedastral):

- **[pkg/adapters](https://pkg.go.dev/github.com/HatiCode/kedastral/pkg/adapters)** - Data source connectors (Prometheus, etc.)
- **[pkg/models](https://pkg.go.dev/github.com/HatiCode/kedastral/pkg/models)** - Forecasting model interfaces and implementations
- **[pkg/capacity](https://pkg.go.dev/github.com/HatiCode/kedastral/pkg/capacity)** - Replica calculation and capacity planning
- **[pkg/storage](https://pkg.go.dev/github.com/HatiCode/kedastral/pkg/storage)** - Forecast snapshot storage backends
- **[pkg/features](https://pkg.go.dev/github.com/HatiCode/kedastral/pkg/features)** - Feature engineering utilities

### View Documentation Locally

```bash
# Install godoc (if not already installed)
go install golang.org/x/tools/cmd/godoc@latest

# Start local documentation server
godoc -http=:6060

# Open in browser
open http://localhost:6060/pkg/github.com/HatiCode/kedastral/
```

---

## ğŸ“Š Observability

### Forecaster Metrics

| Metric | Type | Description |
|---------|------|--------------|
| `kedastral_predicted_value` | Gauge | Current predicted metric value (e.g., RPS) |
| `kedastral_desired_replicas` | Gauge | Current desired replica count (without lead-time) |
| `kedastral_forecast_age_seconds` | Gauge | Age of the current forecast in seconds |
| `kedastral_adapter_collect_seconds` | Histogram | Time spent collecting metrics from adapter |
| `kedastral_model_predict_seconds` | Histogram | Time spent predicting forecast |
| `kedastral_capacity_compute_seconds` | Histogram | Time spent computing desired replicas |
| `kedastral_errors_total` | Counter | Total errors by component and reason |

### Scaler Metrics

| Metric | Type | Description |
|---------|------|--------------|
| `kedastral_scaler_desired_replicas_returned` | Gauge | Last replica count returned to KEDA (with lead-time) |
| `kedastral_scaler_forecast_age_seen_seconds` | Gauge | Age of forecast when scaler fetched it |
| `kedastral_scaler_forecast_fetch_duration_seconds` | Histogram | Time spent fetching forecast from forecaster |
| `kedastral_scaler_forecast_fetch_errors_total` | Counter | Total errors fetching forecasts |
| `kedastral_scaler_grpc_request_duration_seconds` | Histogram | KEDA gRPC request duration by method |

---

## ğŸ§© Extensibility

- **Adapters SDK:** implement your own metric collectors (Go interfaces).
- **Model SDK:** plug in your own forecasting logic.
- **Storage SDK:** replace Redis with your preferred backend.
- **BYOM Mode:** expose an HTTP endpoint returning predictions; Kedastral will use it automatically.

Example interface:
```go
type ForecastModel interface {
    Train(ctx context.Context, data DataFrame) error
    Predict(ctx context.Context, horizon time.Duration) ([]float64, error)
}
```

---

## ğŸ”„ Safety & Fallbacks

- Kedastral can run **hybrid scaling**:
  `effectiveReplicas = max(predicted, reactive)`
  ensuring reactive CPU/RPS-based scaling still applies.
- Built-in clamps: max scale-up/down rate per minute.
- Automatic fallback to KEDAâ€™s default triggers if the forecast is stale or engine is down.

---

## ğŸ§‘â€ğŸ’» Project Goals

1. Provide a **pluggable, open predictive-scaling layer** for Kubernetes.
2. Empower developers to **forecast and pre-scale workloads** in any industry.
3. Keep **data and control within the userâ€™s cluster**.
4. Remain **KEDA-compatible** and **Go-native**.
5. Grow an **open community** around predictive autoscaling research and practice.

---

## ğŸŒ License & Governance

- **License:** Apache-2.0
- **Repository:** `github.com/kedastral/kedastral`
- **Maintainers:** community-governed, CNCF-style steering model.
- **Contributions:** welcomed via PRs, adapters, or model integrations.

---

## ğŸ—ºï¸ Roadmap

| Milestone | Key Features | Status |
|------------|---------------|---------|
| **v0.1** (MVP) | Forecaster + Scaler + Prometheus adapter + baseline model + in-memory storage | âœ… **Complete** |
| **v0.2** | Redis storage + additional ML models (Prophet, ARIMA) + Helm chart | ğŸ”„ Planned |
| **v0.3** | CRDs (ForecastPolicy/DataSource) + Grafana dashboards | ğŸ”„ Planned |
| **v0.4** | Additional adapters (Kafka, HTTP) + BYOM plugin + multi-metric ensembles | ğŸ”„ Planned |
| **v1.0** | Kedastral Operator + model registry + full conformance tests + production hardening | ğŸ”„ Planned |

**v0.1 Deliverables (Current):**
- âœ… Forecaster binary with Prometheus integration
- âœ… Scaler binary implementing KEDA External Scaler protocol
- âœ… Baseline statistical forecasting model
- âœ… In-memory forecast storage
- âœ… Capacity planning with configurable policies
- âœ… Prometheus metrics for observability
- âœ… 81 unit tests
- âœ… Docker support
- âœ… Kubernetes deployment examples
- âœ… Comprehensive documentation

---

## âœ¨ Getting Started

```bash
# 1. Build Kedastral
make build

# 2. Deploy to Kubernetes
kubectl apply -f examples/deployment.yaml
kubectl apply -f examples/scaled-object.yaml

# 3. Monitor forecasts
kubectl logs -l component=forecaster -f
kubectl logs -l component=scaler -f

# 4. Check current forecast
kubectl port-forward svc/kedastral-forecaster 8081:8081
curl "http://localhost:8081/forecast/current?workload=my-api"
```

For detailed instructions, see the [Quick Start](#-quick-start) section above and [examples/README.md](./examples/README.md).

---

## ğŸ§© Summary for AI Assistants

**Project Name:** Kedastral
**Purpose:** Predictive autoscaling framework for Kubernetes built around KEDA
**Core Language:** Go (â‰¥1.25)
**Current Status:** v0.1 MVP - Production-ready core components
**Primary Components:**
- Forecaster (HTTP API, Prometheus integration, baseline model)
- Scaler (gRPC KEDA External Scaler implementation)

**Key Integrations:** KEDA (External Scaler protocol), Prometheus (metrics source)
**Storage:** In-memory (Redis planned for v0.2)
**Domain Scope:** Domain-agnostic (works for any workload)
**Mission:** Enable proactive scaling decisions in Kubernetes through forecasted metrics
**Deployment:** Kubernetes manifests + Docker containers (Helm planned for v0.2)
**Testing:** 81 unit tests covering core functionality
**Architecture Keywords:** predictive autoscaling, statistical forecasting, Kubernetes, Go, gRPC, KEDA External Scaler, Prometheus, time-series prediction, capacity planning, proactive scaling
